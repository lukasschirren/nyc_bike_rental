{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "import folium\n",
    "from folium.plugins import FastMarkerCluster\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "import utm \n",
    "import os\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Generate_basemap():\n",
    "#     basemap = folium.Map(location=[40.730610 , -73.935242])\n",
    "#     return basemap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 List of Stations (Lat,Lon,Capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the JSON file\n",
    "url = 'https://gbfs.lyft.com/gbfs/2.3/bkn/en/station_information.json'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    json_data = response.json()\n",
    "    \n",
    "    # Now you can work with the JSON data)\n",
    "else:\n",
    "    print('Failed to retrieve data:', response.status_code)\n",
    "\n",
    "# Create a DataFrame from the 'stations' list\n",
    "station_data = pd.DataFrame(json_data['data']['stations'])\n",
    "\n",
    "# Select only the required columns\n",
    "station_data = station_data[['short_name', 'name', 'region_id', 'lat', 'lon', 'capacity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Station Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basemap = Generate_basemap()\n",
    "# FastMarkerCluster(station_data[['lat', 'lon' , 'capacity']]).add_to(basemap)\n",
    "\n",
    "# HeatMap(station_data[['lat', 'lon' , 'capacity']]).add_to(basemap)\n",
    "# # basemap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Ride Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Ride Data (2015-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# # Open all files / computing intensive\n",
    "# current_directory = os.getcwd()\n",
    "# file_name = \"data\"\n",
    "# base_folder_path = os.path.join(current_directory, file_name)\n",
    "# start_year = 2015\n",
    "# end_year = 2019\n",
    "# combined_data = combine_csv_files_in_years(base_folder_path,start_year,end_year)\n",
    "# len(combined_data)\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From 2015 - 2021, 15 columns\n",
    "# From 2022 - 2024, 13 columns\n",
    "# Issues with 2017\n",
    "# Issues with 2021\n",
    "\n",
    "# current_directory = os.getcwd()\n",
    "# file_name = \"data//2016\"\n",
    "# folder_path = os.path.join(current_directory, file_name)\n",
    "\n",
    "# data = combine_csv_files(folder_path)\n",
    "# data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_data['starttime'] = pd.to_datetime(combined_data['starttime'])\n",
    "# combined_data['stoptime'] = pd.to_datetime(combined_data['stoptime'])\n",
    "\n",
    "# # Reduce memory usage\n",
    "# cols = ['start station name', 'end station name', 'bikeid', 'usertype', 'gender']\n",
    "# for col in cols:\n",
    "#     combined_data[col] = combined_data[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Ride Data (2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(os.getcwd(), \"data\", \"2020\")\n",
    "\n",
    "# List all files in the folder\n",
    "file_names = os.listdir(folder_path)\n",
    "\n",
    "# Initialize an empty list to store DataFrame chunks\n",
    "chunks = []\n",
    "\n",
    "# Loop through each file, load it, and append it to the list\n",
    "for file_name in file_names:\n",
    "    if file_name.endswith(\".pkl.gz\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        chunk = pd.read_pickle(file_path, compression='gzip')\n",
    "        chunks.append(chunk)\n",
    "\n",
    "# Concatenate all DataFrame chunks into a single DataFrame\n",
    "data_2020 = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_directory = os.getcwd()\n",
    "# file_name = \"data/2020/data_2020.pkl\"\n",
    "# folder_path = os.path.join(current_directory, file_name)\n",
    "\n",
    "# # Load the DataFrame from the pickle file\n",
    "# data_2020 = pd.read_pickle(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the DataFrame into six chunks\n",
    "# chunk_size = len(data_2020) // 6\n",
    "# chunks = [data_2020[i:i+chunk_size] for i in range(0, len(data_2020), chunk_size)]\n",
    "\n",
    "# # Define the folder path where the split files will be saved\n",
    "# folder_path = os.path.join(os.getcwd(), \"data\", \"2020\")\n",
    "\n",
    "# # Save each chunk as a separate gzip-compressed pickle file\n",
    "# for i, chunk in enumerate(chunks):\n",
    "#     chunk_file_name = f\"data_2020_chunk_{i}.pkl.gz\"\n",
    "#     chunk_file_path = os.path.join(folder_path, chunk_file_name)\n",
    "#     chunk.to_pickle(chunk_file_path, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################\n",
    "# current_directory = os.getcwd()\n",
    "# file_name = \"data/2020\"\n",
    "# folder_path = os.path.join(current_directory, file_name)\n",
    "\n",
    "# data_2020 = combine_csv_files(folder_path)\n",
    "# data_2020.iloc[:3]\n",
    "# data_2020['starttime'] = pd.to_datetime(data_2020['starttime'])\n",
    "# data_2020['stoptime'] = pd.to_datetime(data_2020['stoptime'])\n",
    "\n",
    "# # Reduce memory usage\n",
    "# cols = ['start station name', 'end station name', 'bikeid', 'usertype', 'gender']\n",
    "# for col in cols:\n",
    "#     data_2020[col] = data_2020[col].astype('category')\n",
    "\n",
    "# # Define the file path\n",
    "# current_directory = os.getcwd()\n",
    "# file_name = \"data/2020/data_2020.pkl\"\n",
    "# file_path = os.path.join(current_directory, file_name)\n",
    "\n",
    "# # Save the DataFrame as a pickle file\n",
    "# data_2020.to_pickle(file_path)\n",
    "# #############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Ride Data (2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_path = os.path.join(os.getcwd(), \"data\", \"2021\")\n",
    "\n",
    "# # List all files in the folder\n",
    "# file_names = os.listdir(folder_path)\n",
    "\n",
    "# # Initialize an empty list to store DataFrame chunks\n",
    "# chunks = []\n",
    "\n",
    "# # Loop through each file, load it, and append it to the list\n",
    "# for file_name in file_names:\n",
    "#     if file_name.endswith(\".pkl.gz\"):\n",
    "#         file_path = os.path.join(folder_path, file_name)\n",
    "#         chunk = pd.read_pickle(file_path, compression='gzip')\n",
    "#         chunks.append(chunk)\n",
    "\n",
    "# # Concatenate all DataFrame chunks into a single DataFrame\n",
    "# data_2021 = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_directory = os.getcwd()\n",
    "# file_name = \"data/2021/data_2021.pkl\"\n",
    "# folder_path = os.path.join(current_directory, file_name)\n",
    "\n",
    "# # Load the DataFrame from the pickle file\n",
    "# data_2021 = pd.read_pickle(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the DataFrame into six chunks\n",
    "# chunk_size = len(data_2021) // 30\n",
    "# chunks = [data_2021[i:i+chunk_size] for i in range(0, len(data_2021), chunk_size)]\n",
    "\n",
    "# # Define the folder path where the split files will be saved\n",
    "# folder_path = os.path.join(os.getcwd(), \"data\", \"2021\")\n",
    "\n",
    "# # Save each chunk as a separate gzip-compressed pickle file\n",
    "# for i, chunk in enumerate(chunks):\n",
    "#     chunk_file_name = f\"data_2021_chunk_{i}.pkl.gz\"\n",
    "#     chunk_file_path = os.path.join(folder_path, chunk_file_name)\n",
    "#     chunk.to_pickle(chunk_file_path, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# station_data_21 = data_2021[['start_station_name','start_lat','start_lng']]\n",
    "# station_data_21 = station_data_21.drop_duplicates(subset='start_station_name')\n",
    "# station_data_21['capacity'] = 1\n",
    "# # len(station_data_21['start_station_name'].unique())\n",
    "# # len(station_data_21)\n",
    "\n",
    "# station_data_21 = station_data_21.dropna(subset=['start_lat', 'start_lng'])\n",
    "\n",
    "# # Sample data\n",
    "# locations = station_data_21['start_station_name']\n",
    "# latitudes = station_data_21['start_lat']\n",
    "# longitudes = station_data_21['start_lng']\n",
    "# sizes = station_data_21['capacity']\n",
    "# #values = [20, 30, 25, 40]  # Values for the color scale\n",
    "\n",
    "# # Initialize the map centered around the first location\n",
    "# mymap = folium.Map()\n",
    "\n",
    "# # Iterate over locations\n",
    "# for lat, lon, size, location in zip(latitudes, longitudes, sizes, locations):\n",
    "#     # Add circle marker with varying sizes\n",
    "#     folium.CircleMarker(\n",
    "#         location=[lat, lon],\n",
    "#         radius=size / 10,  # Normalize size for better visualization\n",
    "#         color='blue',\n",
    "#         fill=True,\n",
    "#         fill_color='blue',\n",
    "#         fill_opacity=0.6,\n",
    "#         popup=location\n",
    "#     ).add_to(mymap)\n",
    "\n",
    "# # Save the map to an HTML file\n",
    "# mymap.save(\"nyc_bike_stations_21.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Ride Data (2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name = \"data/2024/data_2024.pkl.gz\"\n",
    "folder_path = os.path.join(os.getcwd(), file_name)\n",
    "\n",
    "# Load the DataFrame from the pickle file\n",
    "# data_2019 = pd.read_pickle(folder_path)\n",
    "data_2024 = pd.read_pickle(folder_path, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open only 2024\n",
    "# ############################################\n",
    "# current_directory = os.getcwd()\n",
    "# file_name = \"data/2024\"\n",
    "# folder_path = os.path.join(current_directory, file_name)\n",
    "\n",
    "# data_2024 = combine_csv_files(folder_path)\n",
    "\n",
    "# data_2024['started_at'] = pd.to_datetime(data_2024['started_at'])\n",
    "# data_2024['ended_at'] = pd.to_datetime(data_2024['ended_at'])\n",
    "\n",
    "# cols = ['start_station_name', 'end_station_name', 'start_station_id', 'end_station_id', 'ride_id', 'rideable_type', 'member_casual']\n",
    "# for col in cols:\n",
    "#     data_2024[col] = data_2024[col].astype('category')\n",
    "# ############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_directory = os.getcwd()\n",
    "# file_name = \"data/2024/data_2024.pkl.gz\"\n",
    "# file_path = os.path.join(current_directory, file_name)\n",
    "\n",
    "# # Save the DataFrame as a pickle file\n",
    "# data_2024.to_pickle(file_path, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data_2024['start_lat'] = pd.to_numeric(data_2024[\"start_lat\"])\n",
    "# data_2024['end_lat'] = pd.to_numeric(data_2024[\"end_lat\"])\n",
    "# data_2024['start_lng'] = pd.to_numeric(data_2024[\"start_lng\"])\n",
    "# data_2024['end_lng'] = pd.to_numeric(data_2024[\"end_lng\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4. Ride Data (2019) - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(os.getcwd(), \"data\", \"2019\")\n",
    "\n",
    "# List all files in the folder\n",
    "file_names = os.listdir(folder_path)\n",
    "\n",
    "# Initialize an empty list to store DataFrame chunks\n",
    "chunks = []\n",
    "\n",
    "# Loop through each file, load it, and append it to the list\n",
    "for file_name in file_names:\n",
    "    if file_name.endswith(\".pkl.gz\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        chunk = pd.read_pickle(file_path, compression='gzip')\n",
    "        chunks.append(chunk)\n",
    "\n",
    "# Concatenate all DataFrame chunks into a single DataFrame\n",
    "data_2019 = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the DataFrame into six chunks\n",
    "# chunk_size = len(data_2019) // 6\n",
    "# chunks = [data_2019[i:i+chunk_size] for i in range(0, len(data_2019), chunk_size)]\n",
    "\n",
    "# # Define the folder path where the split files will be saved\n",
    "# folder_path = os.path.join(os.getcwd(), \"data\", \"2019\")\n",
    "\n",
    "# # Save each chunk as a separate gzip-compressed pickle file\n",
    "# for i, chunk in enumerate(chunks):\n",
    "#     chunk_file_name = f\"data_2019_chunk_{i}.pkl.gz\"\n",
    "#     chunk_file_path = os.path.join(folder_path, chunk_file_name)\n",
    "#     chunk.to_pickle(chunk_file_path, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_directory = os.getcwd()\n",
    "# file_name = \"data/2019/data_2019.pkl\"\n",
    "# folder_path = os.path.join(current_directory, file_name)\n",
    "# data_2019.to_pickle(folder_path, compression='gzip')\n",
    "\n",
    "# current_directory = os.getcwd()\n",
    "# file_name = \"data/2019/data_2019.pkl\"\n",
    "# folder_path = os.path.join(current_directory, file_name)\n",
    "\n",
    "# # Load the DataFrame from the pickle file\n",
    "# # data_2019 = pd.read_pickle(folder_path)\n",
    "# data_2019 = pd.read_pickle(folder_path, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open only 2019\n",
    "\n",
    "# current_directory = os.getcwd()\n",
    "# file_name = \"data/2019\"\n",
    "# folder_path = os.path.join(current_directory, file_name)\n",
    "\n",
    "# data_2019 = combine_csv_files(folder_path)\n",
    "# data_2019.iloc[:3]\n",
    "\n",
    "# # Reduce memory usage\n",
    "# data_2019['starttime'] = pd.to_datetime(data_2019['starttime'])\n",
    "# data_2019['stoptime'] = pd.to_datetime(data_2019['stoptime'])\n",
    "\n",
    "# cols = ['start station name', 'end station name', 'bikeid', 'usertype', 'gender']\n",
    "# for col in cols:\n",
    "#     data_2019[col] = data_2019[col].astype('category')\n",
    "\n",
    "# # Define the file path\n",
    "# current_directory = os.getcwd()\n",
    "# file_name = \"data/2019/data_2019.pkl\"\n",
    "# file_path = os.path.join(current_directory, file_name)\n",
    "\n",
    "# # Save the DataFrame as a pickle file\n",
    "# data_2019.to_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessible dataframes    Description                            # of columns  \n",
      "data_2019                Gives data for 2019-Baseline           15\n",
      "data_2024                Gives data for 2024                    13\n",
      "station_data             Existing stations in NYC from Lyft      6\n"
     ]
    }
   ],
   "source": [
    "print(\"Accessible dataframes    Description                            # of columns  \")\n",
    "# print(\"combined_data            Gives combined data from 2015-2019     15\")\n",
    "print(\"data_2019                Gives data for 2019-Baseline           15\")\n",
    "# print(\"data_2020                Gives data for 2020                    15\")\n",
    "# print(\"data_2021                Gives data for 2021                    15\")\n",
    "print(\"data_2024                Gives data for 2024                    13\")\n",
    "print(\"station_data             Existing stations in NYC from Lyft      6\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
